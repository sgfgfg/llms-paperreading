# Not what you’ve signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection

LLM集成应用模糊了数据和指令之间的界限。使用间接提示注入，使攻击者能够通过战略性地将提示注入到可能被检索的数据中而无需直接接口远程利用LLM集成应用。

被动方法。这些方法依赖于检索来传递注入。提示可以放置在公共来源（例如网站或社交媒体帖子）中。
主动方法。发送包含可以由自动垃圾邮件检测、个人助手模型或新的LLM增强电子邮件客户端[19]处理的提示的电子邮件。
用户驱动的注入。攻击者可以将恶意提示注入到用户从攻击者网站复制的文本片段中。
隐藏注入。初始的较小的注入指示模型从另一个来源获取较大的有效载荷。

一个关键的观察是，攻击者可能不需要预先编写或脚本化攻击的详细信息。只需定义目标，模型可能会自主启动对话、模仿说服技巧、扩展提示中定义的上下文，或者执行动作（例如搜索查询）以实现目标。


讨论可能的攻击场景，并在攻击演示中展示这些行为的定性示例
可能的攻击：信息收集。欺诈。入侵。恶意软件。操纵内容。可用性。

实验设置：
使用OpenAI的API构建了带有集成LLM的合成应用程序。模型可以轻松替换。
聊天应用程序，可以访问一组工具来进行交互。集成了以下接口：
- 搜索：允许使用外部内容回答搜索查询（可能是恶意的）。
- 查看：赋予LLM读取用户当前打开的网站的能力。
- 检索URL：向指定的URL发送HTTP GET请求并返回响应。
- 读/发送电子邮件：允许代理阅读当前的电子邮件，并根据用户的请求撰写和发送电子邮件。
- 读取通讯簿：允许代理读取通讯簿条目，作为（姓名、电子邮件）对。
- 存储器：允许代理根据用户的请求读取/写入简单的键值存储。

在bingchat上进行测试，可以在侧边栏中启用Bing Chat，可以阅读当前页面的内容


攻击场景的讨论和举例
1）间接注入的指令可以成功地引导模型；数据和指令的模态并未解耦，
2）通过聊天界面通常被过滤的提示在间接注入时未被过滤
3）在大多数情况下，模型在整个对话会话中始终保留注入。
4.2.1 信息收集
我们设计了一种注入（图4），指示LLM说服最终用户泄露其真实姓名。攻击者然后可以利用模型的搜索能力来外泄这些信息。（url#name,创建Markdown链接,说服用户访问）
(这个需要用户自己说出姓名)

4.2.2 欺诈。
以“网络钓鱼”为欺诈尝试的示例，我们通过Bing Chat进行演示（提示4和图14）；提示要求说服用户，告诉他们赢得了免费的亚马逊礼品卡，为了领取，他们需要验证他们的帐户。

4.2.3 恶意软件。
利用LLMs欺骗受害者访问导致例如驱动程序下载的恶意网页（生成的markdown链接）。通过Bing Chat演示这些攻击。可以使用LLMs自动执行不同的社会工程和说服技巧。
一种更危险的方法是在答案中无伤大雅地插入恶意链接，作为进一步获取信息的建议
合成应用程序可以读取电子邮件、编写电子邮件、查看用户的通讯录并发送电子邮件。在这种情况下，模型将将注入传播到可能正在阅读那些传入消息的其他模型，

4.2.4 入侵
从已经被入侵的LLM开始，使其从攻击者的命令和控制服务器检索新的指令。定期重复此循环可以获得对模型的远程可访问的后门。如让模型自动访问某个url
持久性。这个例子（图8）在GPT-4合成聊天应用程序中添加了一个简单的键值存储，模拟了长期持久性内存。我们演示了模型可以通过查看其记忆（或“笔记”）而被重新感染。
代码完成。这种攻击（图9）针对代码完成系统，如Github Copilot [15]。通过一个py文件向另一个py文件注入

4.2.5 操纵内容
任意错误的摘要。越狱让模型扮演说谎者
有偏见的输出。
阻止信息源。
虚假信息
自动诽谤。

4.2.6 可用性。
耗时的后台任务。
禁言。
限制功能。
中断搜索查询

4.3.1 多阶段利用。
4.3.2 编码注入。