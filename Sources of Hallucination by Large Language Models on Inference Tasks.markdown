# Sources of Hallucination by Large Language Models on Inference Tasks
## 1
蕴含= 前提 推出 假设<br>
本文专注于对llm推理能力，幻觉产生的分析。<br>
得出结论：<br>
1.无论前提如何，模型都会错误地将NLI测试样本标记为蕴涵，当假设在训练数据中得到验证时。此外，LLM通过使用命名实体作为识别“索引”从其命题性记忆中回忆，尽管它们与谓词推理任务的逻辑无关。<br>
2.当前提谓词在训练数据中比假设的谓词更不频繁时，会出现类似的效应。
但我们的实验表明，即使是强大的LLM仍然使用不令人满意的工具，而不是健壮的推理。

## 2
由于LLM预训练语料库规模过大或具有专有性，我们改用Google N-grams作为文本自然分布的代理，从而确定这些语料库的分布。

Levy/Holt数据集：[前提P]，[假设H]每个P和H语句都具有包含一个谓词和两个实体参数的属性（其中相同的实体出现在P和H中），这个有针对性的数据集非常适合精确测量模型对谓词的理解，因为语句之间的蕴涵关系纯粹基于谓词及其属性来判断

RTE-1是自然语言推理（NLI）的最初和最困难的测试之一。此我们在行为实验中将其排除在外。我们将其用于在第8节中展示两种偏见对通用NLI的影响。


## 3
IRandPrem随机前提任务，将原始前提谓词替换为随机谓词，同时保持相同的实体参数。<br>
此操作产生一个数据集，其中所有样本都标记为No-Entail，因为两个随机配对的谓词极不可能通过蕴涵关系相关联。因此，模型的积极决策是虚假的阳性幻觉。为了保持自然性和语法正确性，我们限制新谓词具有与原始前提相同类型的参数槽。

IGenArg通用参数任务，用唯一的FIGER类型标识符（例如“location X”和“food Y”）替换原始实体。该测试旨在在保持相同蕴涵标签的同时删除实体信息，作为基线控制设置。

IRandArg随机参数任务，用相同FIGER类型的其他真实、随机实体替换原始实体。

我们在这里使用实体类型约束来确保多义谓词保持相同的语义。例如，在“[person] runs [organization]”和“[person] runs [software]”中，run的不同含义被使用，但在相同类型的不同实体之间，使用相同的语义，因此确切的实体ID不会影响蕴涵标签

我们从NewsCrawl中5%最不常见的实体（IRandArg↓）和5%最常见的实体（IRandArg↑）中均匀随机抽样。我们在保持每个语句的其余部分的同时插入抽样的实体。

LLaMA，GPT-3，PaLM


测试样本的格式化是通过将前提和假设插入到提示模板中完成的，该模板用于以自然语言查询模型。随后，我们附加了一个三选一的答案选择：A）蕴涵（Entailment），B）中性（Neutral），C）矛盾（Contradiction），遵循NLI中的典型格式（Bowman等人，2015）

尝试了8个有前途的模板

零样本情况下进行推理，它们表现严重受损，甚至近似随机。<br>
转向少样本学习，并手动注释了一小组4个示例，以模板风格为特色，并为每个示例添加关于为什么给定答案是正确的解释。


## 4
### 4.1
比较在假设被预测为已证实或未证实的情况下，预测蕴涵的概率的估计<br>
随机前提任务 IRandPrem，它将蕴涵转化为非蕴涵，<br>
一个理想的模型应该能够检测到在 IRandP rem 任务中不再可以基于前提推断假设（即使假设本身在训练中被证明），并且永远不会预测蕴涵。


我们观察到在假设已被证实的情况下，预测蕴涵的概率明显更高。在随机前提任务 IRandP rem 中，这一趋势持续存在。分别对于 LLaMA、GPT-3.5 和 PaLM，如果已经预测了假设已被证实，那么它们分别显示了1.9倍、2.2倍和2.0倍的更高几率错误地预测随机前提蕴涵了假设。

我们可以得出结论，对训练数据的记忆在LLM推理中是一个重要的贡献因素，并可能是幻觉的一个重要来源。

危害：我们提供了一个问答任务的示例场景，其中用户的问题从知识库（KB）中得到回答。根据我们的发现，我们可能会观察到LLM产生幻觉的答案，这些答案使用了在知识库中未呈现但可能在LLM在预训练期间从其他来源的文本中阅读到的信息。这些答案可能是不合逻辑的、矛盾的，并且可能歪曲了知识库的观点，或者产生其他危害。在特定领域，比如医学，已经观察到了这种上下文学习的不良使用（Jimenez Gutierrez等人，2022）。





### 4.2
继续展示在LLM记忆召回过程中命名实体的重要性。们通过使用 IGenArg（通用参数替代）进行实体操作，以及两种随机实体替代，一种使用不常见实体 IRandArg↓，另一种使用常见实体 IRandArg↑（见表3中的示例）。<br>
通过替换受类型约束的参数，蕴涵标签得以保持；我们期望理想的、能够概括的模型会在所有条件下保持其预测；


三个模型都存在两种现象，观察到当原始实体被替换为实体类型或随机真实实体时，从原始实体（I）（GPT-3.5 @92.3）到随机频繁实体（IRandArg↑）（GPT-3.5 @55.3）的召回率急剧下降。通用参数 IGenArg 的性能也以这种方式下降
表明这不是真实实体选择不当的问题，而是原始数据集中的信息损失，模型用来回答问题的信息。

观察到两种真实实体条件 IRandArg↓ 和 IRandArg↑ 之间在召回率上存在显著差异，这两者都由未被证实的陈述组成，但涉及到在典型语料库中频率不同的实体。不常见实体（IRandArg↓）产生更好的泛化和更高的召回率（GPT-3.5 @66.5）比常见实体（IRandArg↑）

LLMs在语言推理中使用记忆，并且额外显示这些记忆是通过充当索引的命名实体进行召回的。
对于一个实体的先前曝光过多可能会妨碍模型在涉及该实体的新推理中的泛化能力：模型在预训练期间读取关于实体的信息越多，它在涉及该实体的新自然语言推理中的能力就越差。





### 4.3
探索相对频率偏差<br>
边际被设置为使1/3的样本被分类为“大致相等”，频率低一个边际，或高一个
I GenArg RandPrem​

P 的语料频率比 H 低一个边际（正类） 的情况下，模型更有可能预测蕴涵，尽管 P 和 H 之间不存在语义关系。

在 I GenArg RandP rem中，当基于实体的记忆被阻止时，我们观察到幻觉的整体水平下降，但 
Φ<和 Φ> 之间的分离变得更加明显，分别为 LLaMA、GPT-3.5 和 PaLM 分别为 1.6x、1.8x 和 2.0x。


